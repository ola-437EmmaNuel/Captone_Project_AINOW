{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Claim Prediction Model\n",
    "## Predicting Building Insurance Claims Based on Building Characteristics\n",
    "\n",
    "**Objective:** Build a predictive model to determine if a building will have an insurance claim during a certain period.\n",
    "\n",
    "**Target Variable:**\n",
    "- 1: Building has at least one claim over the insured period\n",
    "- 0: Building has no claims over the insured period\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv('Train_data.csv')\n",
    "variable_desc = pd.read_csv('Variable_Description.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nTraining data shape: {train_data.shape}\")\n",
    "print(f\"Number of features: {train_data.shape[1] - 1}\")\n",
    "print(f\"Number of samples: {train_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display variable descriptions\n",
    "print(\"\\n=== VARIABLE DESCRIPTIONS ===\")\n",
    "display(variable_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "print(\"\\n=== FIRST 5 ROWS ===\")\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "print(\"\\n=== DATA INFORMATION ===\")\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n=== STATISTICAL SUMMARY ===\")\n",
    "display(train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "missing_values = train_data.isnull().sum()\n",
    "missing_percent = (missing_values / len(train_data)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "display(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(train_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for categorical columns\n",
    "print(\"\\n=== UNIQUE VALUES IN CATEGORICAL COLUMNS ===\")\n",
    "categorical_cols = train_data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}: {train_data[col].nunique()} unique values\")\n",
    "    print(train_data[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data for cleaning\n",
    "df = train_data.copy()\n",
    "\n",
    "print(\"Original shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handle Customer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer ID is just an identifier, not useful for prediction\n",
    "# We'll drop it but keep it if needed for tracking\n",
    "customer_ids = df['Customer Id'].copy()\n",
    "df = df.drop('Customer Id', axis=1)\n",
    "\n",
    "print(\"Dropped Customer Id column\")\n",
    "print(\"Current shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handle NumberOfWindows Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NumberOfWindows - it appears to have '   .' for missing values\n",
    "print(\"NumberOfWindows unique values:\")\n",
    "print(df['NumberOfWindows'].value_counts().head(10))\n",
    "print(f\"\\nData type: {df['NumberOfWindows'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '   .' with NaN and convert to numeric\n",
    "df['NumberOfWindows'] = df['NumberOfWindows'].replace('   .', np.nan)\n",
    "df['NumberOfWindows'] = pd.to_numeric(df['NumberOfWindows'], errors='coerce')\n",
    "\n",
    "print(f\"Missing values in NumberOfWindows: {df['NumberOfWindows'].isnull().sum()}\")\n",
    "print(f\"Percentage: {(df['NumberOfWindows'].isnull().sum() / len(df)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handle Date_of_Occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age of building feature\n",
    "df['Building_Age'] = df['YearOfObservation'] - df['Date_of_Occupancy']\n",
    "\n",
    "# Check for any negative or unrealistic values\n",
    "print(f\"Building Age Statistics:\")\n",
    "print(df['Building_Age'].describe())\n",
    "print(f\"\\nNegative Building Age: {(df['Building_Age'] < 0).sum()}\")\n",
    "print(f\"Very old buildings (>150 years): {(df['Building_Age'] > 150).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle negative or unrealistic building ages\n",
    "# For negative ages, we'll take the absolute value\n",
    "# For very old buildings, we'll cap at a reasonable value\n",
    "df['Building_Age'] = df['Building_Age'].abs()\n",
    "df['Building_Age'] = df['Building_Age'].clip(upper=200)\n",
    "\n",
    "# Drop original date column as we have Building_Age now\n",
    "df = df.drop('Date_of_Occupancy', axis=1)\n",
    "\n",
    "print(\"Created Building_Age feature and dropped Date_of_Occupancy\")\n",
    "print(f\"Building Age range: {df['Building_Age'].min()} to {df['Building_Age'].max()} years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categorical variables\n",
    "print(\"Categorical variables encoding:\")\n",
    "print(f\"\\nBuilding_Painted: {df['Building_Painted'].unique()}\")\n",
    "print(f\"Building_Fenced: {df['Building_Fenced'].unique()}\")\n",
    "print(f\"Garden: {df['Garden'].unique()}\")\n",
    "print(f\"Settlement: {df['Settlement'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encode categorical variables\n",
    "# Building_Painted: N=Painted, V=Not Painted -> N=1, V=0\n",
    "df['Building_Painted'] = df['Building_Painted'].map({'N': 1, 'V': 0})\n",
    "\n",
    "# Building_Fenced: N=Fenced, V=Not Fenced -> N=1, V=0\n",
    "df['Building_Fenced'] = df['Building_Fenced'].map({'N': 1, 'V': 0})\n",
    "\n",
    "# Garden: V=has garden, O=no garden -> V=1, O=0\n",
    "df['Garden'] = df['Garden'].map({'V': 1, 'O': 0})\n",
    "\n",
    "# Settlement: U=urban, R=rural -> U=1, R=0\n",
    "df['Settlement'] = df['Settlement'].map({'U': 1, 'R': 0})\n",
    "\n",
    "print(\"Categorical variables encoded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current state of the dataset\n",
    "print(\"\\n=== CLEANED DATA INFO ===\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining missing values\n",
    "print(\"\\n=== REMAINING MISSING VALUES ===\")\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "if len(missing) > 0:\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"No missing values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"=== TARGET VARIABLE DISTRIBUTION ===\")\n",
    "claim_counts = df['Claim'].value_counts()\n",
    "claim_percent = df['Claim'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nClaim Distribution:\")\n",
    "print(f\"No Claim (0): {claim_counts[0]} ({claim_percent[0]:.2f}%)\")\n",
    "print(f\"Has Claim (1): {claim_counts[1]} ({claim_percent[1]:.2f}%)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='Claim', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Target Variable Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Claim Status')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['No Claim', 'Has Claim'])\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(claim_counts):\n",
    "    axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(claim_counts, labels=['No Claim', 'Has Claim'], autopct='%1.1f%%', \n",
    "            colors=sns.color_palette('Set2'), startangle=90)\n",
    "axes[1].set_title('Target Variable Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = claim_counts[0] / claim_counts[1]\n",
    "print(f\"\\nClass Imbalance Ratio (No Claim / Has Claim): {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 2 or imbalance_ratio < 0.5:\n",
    "    print(\"⚠️ Dataset shows class imbalance. Consider using techniques like SMOTE or class weights.\")\n",
    "else:\n",
    "    print(\"✓ Dataset is relatively balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols.remove('Claim')  # Remove target variable\n",
    "\n",
    "print(f\"Numerical features: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    if idx < len(axes):\n",
    "        axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'Distribution of {col}', fontweight='bold')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(numerical_cols), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots to detect outliers\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    if idx < len(axes):\n",
    "        sns.boxplot(y=df[col].dropna(), ax=axes[idx], palette='Set2')\n",
    "        axes[idx].set_title(f'Boxplot of {col}', fontweight='bold')\n",
    "        axes[idx].set_ylabel(col)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(numerical_cols), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Relationship with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features vs Target\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    if idx < len(axes):\n",
    "        df_temp = df[[col, 'Claim']].dropna()\n",
    "        sns.boxplot(data=df_temp, x='Claim', y=col, ax=axes[idx], palette='Set2')\n",
    "        axes[idx].set_title(f'{col} vs Claim Status', fontweight='bold')\n",
    "        axes[idx].set_xticklabels(['No Claim', 'Has Claim'])\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(numerical_cols), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs Target\n",
    "categorical_features = ['Residential', 'Building_Painted', 'Building_Fenced', 'Garden', 'Settlement']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    if idx < len(axes):\n",
    "        ct = pd.crosstab(df[col], df['Claim'], normalize='index') * 100\n",
    "        ct.plot(kind='bar', ax=axes[idx], color=['#66c2a5', '#fc8d62'], width=0.7)\n",
    "        axes[idx].set_title(f'{col} vs Claim Rate', fontweight='bold')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Percentage')\n",
    "        axes[idx].legend(['No Claim', 'Has Claim'])\n",
    "        axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(categorical_features), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Matrix of All Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target variable\n",
    "target_corr = correlation_matrix['Claim'].sort_values(ascending=False)\n",
    "print(\"\\n=== CORRELATION WITH TARGET VARIABLE ===\")\n",
    "print(target_corr)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "target_corr[target_corr.index != 'Claim'].plot(kind='barh', color='steelblue')\n",
    "plt.title('Feature Correlation with Claim (Target Variable)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Features')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Additional Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claim rate by Building Type\n",
    "building_type_claim = df.groupby('Building_Type')['Claim'].agg(['sum', 'count', 'mean'])\n",
    "building_type_claim.columns = ['Total_Claims', 'Total_Buildings', 'Claim_Rate']\n",
    "building_type_claim['Claim_Rate'] = building_type_claim['Claim_Rate'] * 100\n",
    "building_type_claim = building_type_claim.sort_values('Claim_Rate', ascending=False)\n",
    "\n",
    "print(\"\\n=== CLAIM RATE BY BUILDING TYPE ===\")\n",
    "display(building_type_claim)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "building_type_claim['Claim_Rate'].plot(kind='bar', color='coral', ax=ax)\n",
    "plt.title('Claim Rate by Building Type', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Building Type')\n",
    "plt.ylabel('Claim Rate (%)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claim rate by Year of Observation\n",
    "year_claim = df.groupby('YearOfObservation')['Claim'].agg(['sum', 'count', 'mean'])\n",
    "year_claim.columns = ['Total_Claims', 'Total_Buildings', 'Claim_Rate']\n",
    "year_claim['Claim_Rate'] = year_claim['Claim_Rate'] * 100\n",
    "\n",
    "print(\"\\n=== CLAIM RATE BY YEAR ===\")\n",
    "display(year_claim)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(year_claim.index, year_claim['Claim_Rate'], marker='o', linewidth=2, markersize=8, color='darkgreen')\n",
    "plt.title('Claim Rate Trend Over Years', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Year of Observation')\n",
    "plt.ylabel('Claim Rate (%)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Building Dimension quartiles and claim rate\n",
    "df['Dimension_Quartile'] = pd.qcut(df['Building Dimension'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "dimension_claim = df.groupby('Dimension_Quartile')['Claim'].mean() * 100\n",
    "\n",
    "print(\"\\n=== CLAIM RATE BY BUILDING DIMENSION QUARTILE ===\")\n",
    "print(dimension_claim)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "dimension_claim.plot(kind='bar', color='purple', alpha=0.7, ax=ax)\n",
    "plt.title('Claim Rate by Building Dimension Quartile', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Building Dimension Quartile')\n",
    "plt.ylabel('Claim Rate (%)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Drop the temporary column\n",
    "df = df.drop('Dimension_Quartile', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Target distribution\n",
    "claim_rate = (df['Claim'].sum() / len(df)) * 100\n",
    "print(f\"\\n1. Overall claim rate: {claim_rate:.2f}%\")\n",
    "\n",
    "# Most correlated features\n",
    "top_corr = target_corr[target_corr.index != 'Claim'].head(3)\n",
    "print(f\"\\n2. Top 3 features correlated with claims:\")\n",
    "for feature, corr in top_corr.items():\n",
    "    print(f\"   - {feature}: {corr:.3f}\")\n",
    "\n",
    "# Missing values summary\n",
    "missing_count = df.isnull().sum().sum()\n",
    "print(f\"\\n3. Total missing values to handle: {missing_count}\")\n",
    "\n",
    "# Building types analysis\n",
    "high_risk_type = building_type_claim.index[0]\n",
    "print(f\"\\n4. Highest risk building type: Type {high_risk_type} ({building_type_claim.loc[high_risk_type, 'Claim_Rate']:.2f}% claim rate)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering and Preprocessing for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for modeling\n",
    "df_model = df.copy()\n",
    "\n",
    "# Feature: Total property features (painted + fenced + garden)\n",
    "df_model['Total_Features'] = (df_model['Building_Painted'].fillna(0) + \n",
    "                               df_model['Building_Fenced'].fillna(0) + \n",
    "                               df_model['Garden'].fillna(0))\n",
    "\n",
    "# Feature: Building dimension categories\n",
    "df_model['Dimension_Category'] = pd.cut(df_model['Building Dimension'], \n",
    "                                         bins=[0, 500, 1000, 2000, float('inf')],\n",
    "                                         labels=[0, 1, 2, 3])\n",
    "df_model['Dimension_Category'] = df_model['Dimension_Category'].astype(int)\n",
    "\n",
    "# Feature: Building age categories\n",
    "df_model['Age_Category'] = pd.cut(df_model['Building_Age'], \n",
    "                                   bins=[0, 20, 50, 100, float('inf')],\n",
    "                                   labels=[0, 1, 2, 3])\n",
    "df_model['Age_Category'] = df_model['Age_Category'].astype(int)\n",
    "\n",
    "# Feature: Insured period categories\n",
    "df_model['InsuredPeriod_Category'] = pd.cut(df_model['Insured_Period'],\n",
    "                                             bins=[0, 0.5, 0.75, 1.0],\n",
    "                                             labels=[0, 1, 2],\n",
    "                                             include_lowest=True)\n",
    "df_model['InsuredPeriod_Category'] = df_model['InsuredPeriod_Category'].astype(int)\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(\"- Total_Features: Sum of painted, fenced, and garden features\")\n",
    "print(\"- Dimension_Category: Building dimension in categories\")\n",
    "print(\"- Age_Category: Building age in categories\")\n",
    "print(\"- InsuredPeriod_Category: Insured period in categories\")\n",
    "\n",
    "print(f\"\\nTotal features now: {df_model.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_cols = df_model.isnull().sum()\n",
    "missing_cols = missing_cols[missing_cols > 0]\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_cols)\n",
    "\n",
    "# Impute NumberOfWindows with median (it's likely missing at random)\n",
    "if 'NumberOfWindows' in missing_cols.index:\n",
    "    median_windows = df_model['NumberOfWindows'].median()\n",
    "    df_model['NumberOfWindows'].fillna(median_windows, inplace=True)\n",
    "    print(f\"\\nImputed NumberOfWindows with median: {median_windows}\")\n",
    "\n",
    "# Check if there are any remaining missing values\n",
    "print(f\"\\nRemaining missing values: {df_model.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_model.drop('Claim', axis=1)\n",
    "y = df_model['Claim']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (80-20 split, stratified to maintain class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTrain set claim rate: {(y_train.sum() / len(y_train)) * 100:.2f}%\")\n",
    "print(f\"Test set claim rate: {(y_test.sum() / len(y_test)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (important for models like Logistic Regression, SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(f\"\\nScaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Building and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss', n_jobs=-1),\n",
    "    'Support Vector Machine': SVC(probability=True, random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(f\"Initialized {len(models)} different models:\")\n",
    "for i, model_name in enumerate(models.keys(), 1):\n",
    "    print(f\"{i}. {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store trained models and results\n",
    "trained_models = {}\n",
    "model_results = []\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Use scaled data for models that benefit from it\n",
    "    if model_name in ['Logistic Regression', 'Support Vector Machine', 'Naive Bayes']:\n",
    "        X_train_use = X_train_scaled\n",
    "        X_test_use = X_test_scaled\n",
    "    else:\n",
    "        X_train_use = X_train\n",
    "        X_test_use = X_test\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_use, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_use)\n",
    "    y_pred_proba = model.predict_proba(X_test_use)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    trained_models[model_name] = model\n",
    "    model_results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"✓ {model_name} trained successfully!\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f} | ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df = results_df.sort_values('ROC-AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "display(results_df.style.background_gradient(cmap='YlGnBu', subset=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    results_sorted = results_df.sort_values(metric, ascending=True)\n",
    "    ax.barh(results_sorted['Model'], results_sorted[metric], color='steelblue')\n",
    "    ax.set_xlabel(metric, fontweight='bold')\n",
    "    ax.set_title(f'Model Comparison: {metric}', fontweight='bold')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Hide the last subplot\n",
    "axes[1, 2].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, model) in enumerate(trained_models.items()):\n",
    "    # Use appropriate data\n",
    "    if model_name in ['Logistic Regression', 'Support Vector Machine', 'Naive Bayes']:\n",
    "        X_test_use = X_test_scaled\n",
    "    else:\n",
    "        X_test_use = X_test\n",
    "    \n",
    "    y_pred = model.predict(X_test_use)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['No Claim', 'Has Claim'],\n",
    "                yticklabels=['No Claim', 'Has Claim'])\n",
    "    axes[idx].set_title(f'{model_name}', fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    # Use appropriate data\n",
    "    if model_name in ['Logistic Regression', 'Support Vector Machine', 'Naive Bayes']:\n",
    "        X_test_use = X_test_scaled\n",
    "    else:\n",
    "        X_test_use = X_test\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test_use)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=2)\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Detailed Report for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model based on ROC-AUC\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use appropriate data\n",
    "if best_model_name in ['Logistic Regression', 'Support Vector Machine', 'Naive Bayes']:\n",
    "    X_test_use = X_test_scaled\n",
    "else:\n",
    "    X_test_use = X_test\n",
    "\n",
    "y_pred_best = best_model.predict(X_test_use)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No Claim', 'Has Claim']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Claim', 'Has Claim'],\n",
    "            yticklabels=['No Claim', 'Has Claim'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Feature Importance (for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "tree_models = ['Decision Tree', 'Random Forest', 'Gradient Boosting', 'XGBoost']\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'Feature': X_train.columns,\n",
    "                'Importance': importances\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\n=== Feature Importance: {model_name} ===\")\n",
    "            display(feature_importance_df.head(10))\n",
    "            \n",
    "            # Visualize\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(feature_importance_df['Feature'].head(10), \n",
    "                    feature_importance_df['Importance'].head(10),\n",
    "                    color='teal')\n",
    "            plt.xlabel('Importance', fontweight='bold')\n",
    "            plt.title(f'Top 10 Feature Importances - {model_name}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.grid(alpha=0.3, axis='x')\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Cross-Validation for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the best model\n",
    "print(f\"\\nPerforming 5-fold cross-validation on {best_model_name}...\")\n",
    "\n",
    "# Use appropriate data\n",
    "if best_model_name in ['Logistic Regression', 'Support Vector Machine', 'Naive Bayes']:\n",
    "    X_cv = scaler.fit_transform(X)\n",
    "else:\n",
    "    X_cv = X\n",
    "\n",
    "cv_scores = cross_val_score(best_model, X_cv, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"\\nCross-Validation ROC-AUC Scores: {cv_scores}\")\n",
    "print(f\"Mean ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Visualize CV scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), cv_scores, marker='o', linestyle='-', linewidth=2, markersize=10)\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Mean: {cv_scores.mean():.4f}')\n",
    "plt.xlabel('Fold Number', fontweight='bold')\n",
    "plt.ylabel('ROC-AUC Score', fontweight='bold')\n",
    "plt.title(f'Cross-Validation Scores - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROJECT SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. BEST PERFORMING MODEL: {best_model_name}\")\n",
    "print(f\"   - Test Set ROC-AUC: {results_df.iloc[0]['ROC-AUC']:.4f}\")\n",
    "print(f\"   - Test Set Accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\n",
    "print(f\"   - Cross-Validation ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\n2. TOP 3 MODELS:\")\n",
    "for i in range(min(3, len(results_df))):\n",
    "    print(f\"   {i+1}. {results_df.iloc[i]['Model']}: ROC-AUC = {results_df.iloc[i]['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. KEY INSIGHTS:\")\n",
    "print(f\"   - Dataset contains {len(df)} buildings with {X.shape[1]} features\")\n",
    "print(f\"   - Overall claim rate: {(y.sum() / len(y)) * 100:.2f}%\")\n",
    "print(f\"   - Class distribution maintained in train-test split\")\n",
    "\n",
    "print(f\"\\n4. RECOMMENDATIONS:\")\n",
    "print(f\"   - Deploy {best_model_name} for production predictions\")\n",
    "print(f\"   - Monitor model performance regularly and retrain with new data\")\n",
    "print(f\"   - Consider collecting more data for buildings with claims to improve recall\")\n",
    "print(f\"   - Implement feature monitoring to detect data drift\")\n",
    "print(f\"   - Consider ensemble methods combining top 3 models for robust predictions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the best model\n",
    "# import joblib\n",
    "# joblib.dump(best_model, 'best_insurance_claim_model.pkl')\n",
    "# joblib.dump(scaler, 'scaler.pkl')\n",
    "# print(f\"Best model ({best_model_name}) saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Project Checklist ✓\n",
    "\n",
    "✅ **Data Cleaning and Preprocessing**\n",
    "- Handled missing values in NumberOfWindows\n",
    "- Encoded categorical variables\n",
    "- Created Building_Age feature from Date_of_Occupancy\n",
    "- Removed identifier column (Customer Id)\n",
    "\n",
    "✅ **Exploratory Data Analysis**\n",
    "- Target variable distribution analysis\n",
    "- Numerical features distribution and outlier detection\n",
    "- Correlation analysis\n",
    "- Feature relationship with target variable\n",
    "- Claim rate analysis by building type, year, and dimensions\n",
    "\n",
    "✅ **Preprocessing for Modeling**\n",
    "- Feature engineering (Total_Features, Dimension_Category, Age_Category)\n",
    "- Train-test split with stratification\n",
    "- Feature scaling with StandardScaler\n",
    "- Proper handling of missing values\n",
    "\n",
    "✅ **Multiple Model Implementation**\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "- Support Vector Machine\n",
    "- Naive Bayes\n",
    "\n",
    "✅ **Model Evaluation**\n",
    "- Comprehensive metrics (Accuracy, Precision, Recall, F1-Score, ROC-AUC)\n",
    "- Confusion matrices for all models\n",
    "- ROC curves comparison\n",
    "- Feature importance analysis\n",
    "- Cross-validation for best model\n",
    "- Detailed classification report\n",
    "\n",
    "---\n",
    "\n",
    "**GitHub Repository Structure Recommendation:**\n",
    "```\n",
    "insurance-claim-prediction/\n",
    "│\n",
    "├── README.md\n",
    "├── insurance_claim_prediction.ipynb\n",
    "├── data/\n",
    "│   ├── Train_data.csv\n",
    "│   └── Variable_Description.csv\n",
    "├── models/\n",
    "│   ├── best_model.pkl\n",
    "│   └── scaler.pkl\n",
    "└── requirements.txt\n",
    "```\n",
    "\n",
    "**README.md should include:**\n",
    "- Project objective and description\n",
    "- Dataset information\n",
    "- Installation instructions\n",
    "- Usage guide\n",
    "- Model performance summary\n",
    "- Key findings and insights\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
